{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7494bd9",
   "metadata": {},
   "source": [
    "# YOLOv8 Face Recognition Training Notebook\n",
    "\n",
    "This notebook trains a YOLOv8 (classification mode) model for celebrity face recognition using the folder structure under `dataset/` (train/test per class). It:\n",
    "\n",
    "- Installs required packages\n",
    "- Detects GPU (CUDA) automatically\n",
    "- Loads images via `torchvision.datasets.ImageFolder`\n",
    "- Applies resizing & normalization transforms compatible with YOLOv8 classification (224x224)\n",
    "- Trains for 20 epochs tracking Accuracy, Precision, Recall, F1, Loss\n",
    "- Saves per-epoch model checkpoints & plots into a timestamped subfolder inside `results/`\n",
    "- Selects & saves the best model (`best_model.pt`)\n",
    "- Evaluates on the test set and produces a percentage confusion matrix.\n",
    "\n",
    "Run cells in order. Adjust batch size or learning rate if you encounter memory limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf234c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (8.3.181)\n",
      "Requirement already satisfied: torch in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: torchvision in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (0.23.0)\n",
      "Requirement already satisfied: torchaudio in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: tqdm in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: seaborn in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: scikit-learn in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: matplotlib in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: numpy>=1.23.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (2.2.6)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (4.12.0.88)\n",
      "Requirement already satisfied: pillow>=7.1.2 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (11.3.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (2.32.5)\n",
      "Requirement already satisfied: scipy>=1.4.1 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (1.16.1)\n",
      "Requirement already satisfied: psutil in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (2.3.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from ultralytics) (2.0.15)\n",
      "Requirement already satisfied: filelock in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: colorama in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from matplotlib) (4.59.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\work\\yolo-cnn\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages (run once). If already installed, can skip.\n",
    "%pip install ultralytics torch torchvision torchaudio tqdm seaborn scikit-learn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed43ce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Run directory: results\\20250819_192427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Yolo-CNN\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports, configuration, CUDA check, results directory setup\n",
    "import os, math, time, json, random, shutil, datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# Create timestamped run directory\n",
    "RESULTS_ROOT = Path('results')\n",
    "RESULTS_ROOT.mkdir(exist_ok=True)\n",
    "run_timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "RUN_DIR = RESULTS_ROOT / run_timestamp\n",
    "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Run directory:', RUN_DIR)\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32  # adjust if out of memory\n",
    "EPOCHS = 20\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee3aa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes (31): ['Akshay Kumar', 'Alexandra Daddario', 'Alia Bhatt', 'Amitabh Bachchan', 'Andy Samberg', 'Anushka Sharma', 'Billie Eilish', 'Brad Pitt', 'Camila Cabello', 'Charlize Theron', 'Claire Holt', 'Courtney Cox', 'Dwayne Johnson', 'Elizabeth Olsen', 'Ellen Degeneres', 'Henry Cavill', 'Hrithik Roshan', 'Hugh Jackman', 'Jessica Alba', 'Kashyap', 'Lisa Kudrow', 'Margot Robbie', 'Marmik', 'Natalie Portman', 'Priyanka Chopra', 'Robert Downey Jr', 'Roger Federer', 'Tom Cruise', 'Vijay Deverakonda', 'Virat Kohli', 'Zac Efron']\n",
      "Saved class names to results\\20250819_192427\\classes.json\n"
     ]
    }
   ],
   "source": [
    "# Dataset & DataLoaders\n",
    "DATA_ROOT = Path('dataset')\n",
    "train_dir = DATA_ROOT / 'train'\n",
    "val_dir = DATA_ROOT / 'test'  # using provided test as validation/eval\n",
    "\n",
    "# Transforms (YOLOv8 classification defaults roughly: resize + center crop + normalize ImageNet stats)\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
    "])\n",
    "val_tfms = train_tfms\n",
    "\n",
    "train_ds = datasets.ImageFolder(root=str(train_dir), transform=train_tfms)\n",
    "val_ds = datasets.ImageFolder(root=str(val_dir), transform=val_tfms)\n",
    "\n",
    "class_names = train_ds.classes\n",
    "num_classes = len(class_names)\n",
    "print(f'Classes ({num_classes}):', class_names)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=device.type=='cuda')\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=device.type=='cuda')\n",
    "\n",
    "with open(RUN_DIR / 'classes.json', 'w') as f:\n",
    "    json.dump(class_names, f, indent=2)\n",
    "print('Saved class names to', RUN_DIR / 'classes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c10f020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced terminal Linear model.9.linear: 1280 -> 31\n",
      "Model ready with 31 classes. Head replacement strategy success = True\n"
     ]
    }
   ],
   "source": [
    "# Model preparation (YOLOv8 classification)\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load pretrained YOLOv8n classification weights\n",
    "base_model = YOLO('yolov8n-cls.pt')\n",
    "\n",
    "# Access the underlying torch ClassificationModel\n",
    "model = base_model.model  # ClassificationModel\n",
    "\n",
    "# --- Flexible classifier adaptation strategies ---\n",
    "# Strategy 1: Use reset_classifier if available (some versions implement this)\n",
    "reset_done = False\n",
    "if hasattr(model, 'reset_classifier'):\n",
    "    try:\n",
    "        model.reset_classifier(num_classes=num_classes)\n",
    "        print('Used model.reset_classifier to set num_classes =', num_classes)\n",
    "        reset_done = True\n",
    "    except Exception as e:\n",
    "        print('reset_classifier failed:', e)\n",
    "\n",
    "# Strategy 2: Replace last nn.Linear whose out_features looks like original class count (e.g., 1000) inside model.model (ModuleList)\n",
    "if not reset_done and hasattr(model, 'model') and isinstance(model.model, (nn.Sequential, nn.ModuleList)):\n",
    "    candidate_indices = [i for i, m in reversed(list(enumerate(model.model))) if isinstance(m, nn.Linear)]\n",
    "    replaced = False\n",
    "    for idx in candidate_indices:\n",
    "        lin = model.model[idx]\n",
    "        in_features = lin.in_features\n",
    "        if lin.out_features != num_classes:\n",
    "            try:\n",
    "                model.model[idx] = nn.Linear(in_features, num_classes)\n",
    "                print(f'Replaced Linear at index {idx}: {in_features} -> {num_classes}')\n",
    "                replaced = True\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f'Failed replacing Linear at index {idx}:', e)\n",
    "    if replaced:\n",
    "        reset_done = True\n",
    "\n",
    "# Strategy 3: Search all submodules for a terminal Linear layer with large out_features (e.g., >= num_classes)\n",
    "if not reset_done:\n",
    "    term_linear = None\n",
    "    for name, module_sub in reversed(list(model.named_modules())):\n",
    "        if isinstance(module_sub, nn.Linear):\n",
    "            term_linear = (name, module_sub)\n",
    "            break\n",
    "    if term_linear is not None:\n",
    "        name, lin = term_linear\n",
    "        if lin.out_features != num_classes:\n",
    "            in_features = lin.in_features\n",
    "            # Replace via attribute traversal\n",
    "            parent = model\n",
    "            name_parts = name.split('.')\n",
    "            for p in name_parts[:-1]:\n",
    "                parent = getattr(parent, p)\n",
    "            try:\n",
    "                setattr(parent, name_parts[-1], nn.Linear(in_features, num_classes))\n",
    "                print(f'Replaced terminal Linear {name}: {in_features} -> {num_classes}')\n",
    "                reset_done = True\n",
    "            except Exception as e:\n",
    "                print(f'Failed replacing terminal Linear {name}:', e)\n",
    "        else:\n",
    "            print('Existing terminal Linear already matches num_classes.')\n",
    "            reset_done = True\n",
    "\n",
    "# Strategy 4: Wrap model with a new head if no internal Linear layer was found/replaced.\n",
    "if not reset_done:\n",
    "    print('No suitable internal Linear layer found; wrapping with new head.')\n",
    "    class YOLOClassifierWrapper(nn.Module):\n",
    "        def __init__(self, backbone, num_classes, img_size):\n",
    "            super().__init__()\n",
    "            self.backbone = backbone\n",
    "            self.num_classes = num_classes\n",
    "            self.img_size = img_size\n",
    "            with torch.no_grad():\n",
    "                dummy = torch.randn(1, 3, img_size, img_size)\n",
    "                feat = self.backbone(dummy)\n",
    "                if isinstance(feat, (list, tuple)):\n",
    "                    feat = feat[0]\n",
    "                # If 4D, global average pool to (N, C)\n",
    "                if feat.ndim == 4:\n",
    "                    feat = feat.mean(dim=(2,3))\n",
    "                self.feat_dim = feat.shape[1]\n",
    "            self.head = nn.Linear(self.feat_dim, num_classes)\n",
    "        def forward(self, x):\n",
    "            out = self.backbone(x)\n",
    "            if isinstance(out, (list, tuple)):\n",
    "                out = out[0]\n",
    "            if out.ndim == 4:\n",
    "                out = out.mean(dim=(2,3))\n",
    "            # If out already equals desired num_classes, assume backbone handled it\n",
    "            if out.shape[1] != self.num_classes:\n",
    "                out = self.head(out)\n",
    "            return out\n",
    "    model = YOLOClassifierWrapper(model, num_classes, IMG_SIZE)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print('Model ready with', num_classes, 'classes. Head replacement strategy success =', reset_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb6c62ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: loss=2.5482 acc=0.3556 prec=0.3791 rec=0.3239 f1=0.3206 | best_f1=0.3206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: loss=1.5230 acc=0.6526 prec=0.6621 rec=0.6268 f1=0.6298 | best_f1=0.6298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: loss=1.1249 acc=0.7557 prec=0.7595 rec=0.7448 f1=0.7479 | best_f1=0.7479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: loss=0.9130 acc=0.8177 prec=0.8285 rec=0.8117 f1=0.8168 | best_f1=0.8168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: loss=0.7547 acc=0.8493 prec=0.8554 rec=0.8478 f1=0.8502 | best_f1=0.8502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: loss=0.6354 acc=0.8790 prec=0.8836 rec=0.8807 f1=0.8814 | best_f1=0.8814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: loss=0.5597 acc=0.9020 prec=0.9063 rec=0.9014 f1=0.9033 | best_f1=0.9033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: loss=0.5100 acc=0.9094 prec=0.9141 rec=0.9113 f1=0.9118 | best_f1=0.9118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: loss=0.4588 acc=0.9227 prec=0.9330 rec=0.9229 f1=0.9265 | best_f1=0.9265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: loss=0.3783 acc=0.9477 prec=0.9516 rec=0.9514 f1=0.9511 | best_f1=0.9511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: loss=0.3414 acc=0.9586 prec=0.9614 rec=0.9597 f1=0.9602 | best_f1=0.9602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: loss=0.3147 acc=0.9586 prec=0.9606 rec=0.9610 f1=0.9604 | best_f1=0.9604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: loss=0.2733 acc=0.9696 prec=0.9713 rec=0.9702 f1=0.9705 | best_f1=0.9705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: loss=0.2486 acc=0.9738 prec=0.9741 rec=0.9745 f1=0.9741 | best_f1=0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: loss=0.2331 acc=0.9770 prec=0.9776 rec=0.9780 f1=0.9777 | best_f1=0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: loss=0.2212 acc=0.9805 prec=0.9820 rec=0.9819 f1=0.9818 | best_f1=0.9818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: loss=0.1859 acc=0.9856 prec=0.9862 rec=0.9865 f1=0.9863 | best_f1=0.9863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: loss=0.1736 acc=0.9902 prec=0.9909 rec=0.9908 f1=0.9907 | best_f1=0.9907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: loss=0.1641 acc=0.9926 prec=0.9932 rec=0.9933 f1=0.9932 | best_f1=0.9932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: loss=0.1682 acc=0.9902 prec=0.9907 rec=0.9910 f1=0.9908 | best_f1=0.9932\n",
      "Training complete. Best model saved to results\\20250819_192427\\best_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "metrics_history = {\n",
    "    'epoch': [],\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'train_precision': [],\n",
    "    'train_recall': [],\n",
    "    'train_f1': []\n",
    "}\n",
    "\n",
    "best_f1 = -1.0\n",
    "best_path = RUN_DIR / 'best_model.pt'\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}/{EPOCHS}', leave=False)\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        if isinstance(outputs, (list, tuple)):\n",
    "            outputs = outputs[0]\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.append(preds.detach().cpu())\n",
    "        all_targets.append(targets.detach().cpu())\n",
    "\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    all_preds = torch.cat(all_preds).numpy()\n",
    "    all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "    train_loss = float(np.mean(epoch_losses))\n",
    "    train_acc = accuracy_score(all_targets, all_preds)\n",
    "    train_precision = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    train_recall = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "    train_f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    metrics_history['epoch'].append(epoch)\n",
    "    metrics_history['train_loss'].append(train_loss)\n",
    "    metrics_history['train_acc'].append(train_acc)\n",
    "    metrics_history['train_precision'].append(train_precision)\n",
    "    metrics_history['train_recall'].append(train_recall)\n",
    "    metrics_history['train_f1'].append(train_f1)\n",
    "\n",
    "    # Save checkpoint\n",
    "    epoch_ckpt = RUN_DIR / f'trained_epoch_{epoch}.pt'\n",
    "    torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'optimizer_state': optimizer.state_dict(), 'metrics': metrics_history}, epoch_ckpt)\n",
    "\n",
    "    # Plot metrics (acc/prec/recall/f1)\n",
    "    fig1, ax1 = plt.subplots(figsize=(8,5))\n",
    "    ax1.plot(metrics_history['epoch'], metrics_history['train_acc'], label='Accuracy')\n",
    "    ax1.plot(metrics_history['epoch'], metrics_history['train_precision'], label='Precision')\n",
    "    ax1.plot(metrics_history['epoch'], metrics_history['train_recall'], label='Recall')\n",
    "    ax1.plot(metrics_history['epoch'], metrics_history['train_f1'], label='F1')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Score')\n",
    "    ax1.set_title('Training Metrics')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    fig1.tight_layout()\n",
    "    fig1.savefig(RUN_DIR / f'trained_epoch_{epoch}_metrics.png')\n",
    "    plt.close(fig1)\n",
    "\n",
    "    # Plot loss\n",
    "    fig2, ax2 = plt.subplots(figsize=(8,5))\n",
    "    ax2.plot(metrics_history['epoch'], metrics_history['train_loss'], label='Loss', color='red')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_title('Training Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    fig2.tight_layout()\n",
    "    fig2.savefig(RUN_DIR / f'trained_epoch_{epoch}_loss.png')\n",
    "    plt.close(fig2)\n",
    "\n",
    "    # Update best model\n",
    "    if train_f1 > best_f1:\n",
    "        best_f1 = train_f1\n",
    "        torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'metrics': metrics_history}, best_path)\n",
    "\n",
    "    print(f'Epoch {epoch}: loss={train_loss:.4f} acc={train_acc:.4f} prec={train_precision:.4f} rec={train_recall:.4f} f1={train_f1:.4f} | best_f1={best_f1:.4f}')\n",
    "\n",
    "print('Training complete. Best model saved to', best_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5078e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: acc=0.9984 precision=0.9986 recall=0.9985 f1=0.9985\n",
      "Saved confusion matrix to results\\20250819_192427\\confusion_matrix.png\n",
      "Saved final metrics JSON.\n",
      "Saved confusion matrix to results\\20250819_192427\\confusion_matrix.png\n",
      "Saved final metrics JSON.\n"
     ]
    }
   ],
   "source": [
    "# Test evaluation & confusion matrix\n",
    "# Load best model\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(ckpt['model_state'])\n",
    "model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(val_loader, desc='Testing', leave=False):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        outputs = model(images)\n",
    "        if isinstance(outputs, (list, tuple)):\n",
    "            outputs = outputs[0]\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_targets.append(targets.cpu())\n",
    "\n",
    "all_preds = torch.cat(all_preds).numpy()\n",
    "all_targets = torch.cat(all_targets).numpy()\n",
    "\n",
    "overall_acc = accuracy_score(all_targets, all_preds)\n",
    "overall_precision = precision_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "overall_recall = recall_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "overall_f1 = f1_score(all_targets, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "print(f'Test: acc={overall_acc:.4f} precision={overall_precision:.4f} recall={overall_recall:.4f} f1={overall_f1:.4f}')\n",
    "\n",
    "cm = confusion_matrix(all_targets, all_preds, labels=list(range(num_classes)))\n",
    "cm_percent = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n",
    "cm_percent = np.nan_to_num(cm_percent)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(max(8, num_classes*0.4), max(6, num_classes*0.4)))\n",
    "sns.heatmap(cm_percent*100, annot=False, cmap='Blues', cbar=True, ax=ax, fmt='.1f')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "ax.set_title('Confusion Matrix (%)')\n",
    "ax.set_xticks(np.arange(num_classes)+0.5)\n",
    "ax.set_yticks(np.arange(num_classes)+0.5)\n",
    "ax.set_xticklabels(class_names, rotation=90)\n",
    "ax.set_yticklabels(class_names, rotation=0)\n",
    "fig.tight_layout()\n",
    "cm_path = RUN_DIR / 'confusion_matrix.png'\n",
    "fig.savefig(cm_path, dpi=150)\n",
    "plt.close(fig)\n",
    "print('Saved confusion matrix to', cm_path)\n",
    "\n",
    "# Save final metrics JSON\n",
    "final_metrics = {\n",
    "    'overall_acc': overall_acc,\n",
    "    'overall_precision': overall_precision,\n",
    "    'overall_recall': overall_recall,\n",
    "    'overall_f1': overall_f1,\n",
    "    'best_epoch': ckpt['epoch']\n",
    "}\n",
    "with open(RUN_DIR / 'final_metrics.json', 'w') as f:\n",
    "    json.dump(final_metrics, f, indent=2)\n",
    "print('Saved final metrics JSON.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c20976",
   "metadata": {},
   "source": [
    "## Training Run Summary\n",
    "\n",
    "Artifacts saved under the timestamped folder in `results/` (value stored in `RUN_DIR`).\n",
    "\n",
    "Contents include:\n",
    "- `trained_epoch_{n}.pt` per-epoch checkpoints\n",
    "- `trained_epoch_{n}_metrics.png` line plots for accuracy/precision/recall/F1\n",
    "- `trained_epoch_{n}_loss.png` loss curve\n",
    "- `best_model.pt` best checkpoint by F1\n",
    "- `classes.json` class label order\n",
    "- `confusion_matrix.png` final confusion matrix (percentage per true class)\n",
    "- `final_metrics.json` overall test metrics and best epoch\n",
    "\n",
    "You can start another training run by re-running from the imports cell (a new timestamped folder will be created). Adjust hyperparameters as needed (e.g., `BATCH_SIZE`, `LR`, `EPOCHS`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
